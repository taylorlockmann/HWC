---
title: "HWC"
author: "Roshni Katrak-Adefowora"
date: "11/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load packages
library(tidyverse)
library(rgbif)
library(maptools)
library(dismo)
library(rgeos)
library(viridis)
#library(scrubr)
library(raster)
library(DHARMa)
library(spocc)
library(sf)
library(rgdal)
library(spData)
library(here)

#install wallace

#load Wallace functions
source(system.file('shiny/funcs', 'functions.R', package = 'wallace'))
```

## Description:

This markdown file outlines methods that can be used to create data for species distribution modeling (SDM) with the Maxent GUI.

The Maxent GUI can be run two different ways:

  1) Using samples with data (SWD) where you extract the climate data from the bioclim rasters for each occurrence and background point.
  
  2) Using lat/long for each point and the bioclim rasters.

### Download species occurence data and get coordinates

For both steps, you will need to begin by downloading the species occurrence points, which can be done 3 different ways 1) with the function from Wallace, 2) with the function from the rgbif packages, or 3) by downloading from gbif.org itself.

We chose to go forward with the function from Wallace.

#### 1) Download using Wallace's spocc:occ() function

Gives global records.  If limit is set high, this function can take a while to run.

```{r}
results <- spocc::occ(query = "Panthera leo", # scientific name, here we are using lions
                      from = "gbif", # set to records from https://www.gbif.org/
                      limit = 15000, # max number of records
                      has_coords = TRUE) # gets the lat/long for each observation

# select just GBIF records, format the species name
results[["gbif"]]$data[[formatSpName("Panthera leo")]]

# select just the necessary information
myspecies_coords <- as.data.frame(results$gbif$data$Panthera_leo) %>% # using GBIF data
  dplyr::select(longitude, latitude, occurrenceStatus, coordinateUncertaintyInMeters, institutionCode, references, basisOfRecord) %>%  # selecting just the columns that we want
  filter(!basisOfRecord %in% c("FOSSIL_SPECIMEN", "PRESERVED_SPECIMEN"),
         occurrenceStatus == "PRESENT") # would we want to set occruanceStatus to just present?
```

#### 2) Download using the occ_data function

Returns records for Africa only

```{r}
#can use occ_data function for GBIF
 results_africa <- occ_data(scientificName = "Panthera leo",
                       hasCoordinate = TRUE,
                       continent = "Africa")

 myspecies_coords_africa <- results_africa$data %>%
   rename(latitude = decimalLatitude) %>%
   rename(longitude = decimalLongitude)
```

### Clean Data

```{r}
# remove rows with duplicate coordinates
occs.dups <- duplicated(myspecies_coords[c('longitude', 'latitude')])
myspecies_coords <- myspecies_coords[!occs.dups,]

# make sure latitude and longitude are numeric (sometimes they are characters)
myspecies_coords$latitude <- as.numeric(myspecies_coords$latitude)
myspecies_coords$longitude <- as.numeric(myspecies_coords$longitude)

# give all records a unique ID
myspecies_coords$occID <- row.names(myspecies_coords)

# removing unlikely, imprecise, incomplete, and impossible coordinates
#occs <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))

# # save lat/long points as a csv if needed
# occs_cleaned <- coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))%>%
#   rename(lat = latitude) %>% # renaming lat/long so it works with projection later on?
#   rename(long = longitude)
# lat_long <- lion_cleaned %>% 
#   dplyr::select(lat, long)
# 
# write_csv(lat_long, "occs_cleaned_coords.csv")
```

### Crop data using Africa boundary

We decided to use a shapefile of Africa instead of selecting with a polygon on Wallace because it's easier and cleaner.

```{r}
# load world data from spData package
world_sp <-  as(world, "Spatial")
world_sf <-  st_as_sf(world_sp, "sf")

# filter countries to just those in Africa (removing Madagascar)
africa <- world_sf %>% 
  filter(continent == "Africa") %>% 
  filter(name_long != "Madagascar") %>% 
  dplyr::select(name_long)

# merge all the country polygons to create one large continental polygon
africa_2 <- st_combine(africa)

# convert to a spatial polygon
africa_spatial <- as_Spatial(africa_2, cast = TRUE)

# creates new data frame with observation data lat/long points
occs.xy <- myspecies_coords[c('longitude', 'latitude')] #change occs to myspecies_coords for now since can't install "scrubr"

# convert species data to spatial layer
coordinates(occs.xy) <- ~longitude+latitude
projection(occs.xy) <- CRS('+proj=longlat +datum=WGS84')

# double-check that polygon and points have the same projection
crs(africa_spatial) # +proj=longlat +datum=WGS84 +no_defs 
crs(occs.xy) # +proj=longlat +datum=WGS84 +no_defs 

# overlays those points over the polygon
intersect <- sp::over(occs.xy, africa_spatial)
# could also use 
# raster::crop(occs.xy, africa_spatial)

# removes na values, so selects for points where they overlap the focal polygon
intersect.rowNums <- as.numeric(which(!(is.na(intersect))))

# filters our occurrence data to those overlapping points
occs <- myspecies_coords[intersect.rowNums, ] #again, change occs to myspecies_coords
occs$name <- "Panthera leo"
```

#### In case you want to create a map figure to double check everything
```{r}
# world map example

(data("wrld_simpl"))

# run plot and points function at once
plot(wrld_simpl, 
     xlim = range(occs$long),
     ylim = range(occs$lat), 
     axes = TRUE, 
     col = "light yellow"
     )
points(occs$longitude, occs$latitude, col = 'blue', pch = 20, cex = 0.75) #add the points
```

### Spatial thin

This will remove occurrence points that are within a specified distance from one-another, and is very important when dealing with areas that have a disproportionate amount of observations in a small area (like in wildlife reserves and parks, which we observe in Africa). It may not necessarily reflect that the climate in that area is ideally suited for that organism. For us, it may be a reflection of reduced human-impacts and an increase in reported occurrences by visitors.

```{r}
# just doing 10 replicates for now, thinning to 10 km
output <- spThin::thin(occs, 'latitude', 'longitude', 'name', 
                       thin.par = 10, # thinned to 10 km, but probably want to increase this by a lot!
                       reps = 10, # default on Wallace is 100, but run time is long, so a low number is good for testing
                       locs.thinned.list.return = TRUE, 
                       write.files = FALSE, 
                       verbose = FALSE)

# find the iteration that returns the max number of occurrences
maxThin <- which(sapply(output, nrow) == max(sapply(output, nrow)))

# if there's more than one max, pick the first one
maxThin <- output[[ifelse(length(maxThin) > 1, maxThin[1], maxThin)]]  

# subset data to those thinned records
occs <- occs[as.numeric(rownames(maxThin)),]

# save occs to upload to Wallace if needed
 occs_wallace <- occs %>% 
   dplyr::select(name, longitude, latitude) # change order to match
 write_csv(occs_wallace, here("outputs", "thinned_data_for_Wallace.csv"))
```

### Create background points

There are many ways to create background points. We tested creating background points with 1) one background point per pixel, 2) create background points within occurrence point buffers, and 3) using minimum convex polygons (following Wallace)

#### 2) Sample from witin buffers around occurrence points 

```{r}
#bgExt <- rgeos::gBuffer(bgExt, width = 0.5)

# create point buffers of 50 km
buffer_points <- circles(occs, d = 50000, lonlat = TRUE)

# getting random points from within our polygons
bg_points <-  spsample(buffer_points@polygons, 1000, type = 'random', iter = 1000)

```

#### 3) Minimum convex polygon

This method is good if you want to test how different number of background points impacts results, but we don't think it's the best because we're looking over such a large area.

This method is following Wallace, and needs the environmental data to be loaded first.

```{r}
# get WorldClim bioclimatic variable rasters
envs <- raster::getData(name = "worldclim", var = "bio", res = 2.5, lat = , lon = )

# change names rasters variables
envRes <- 2.5 # our chosen resolution?

if (envRes == 0.5) { # this code will only run if we chose resolution of 0.5
  i <- grep('_', names(envs))
  editNames <- sapply(strsplit(names(envs)[i], '_'), function(x) x[1])
  names(envs)[i] <- editNames
}

# makes an integer with saving all of our chosen bioclim variables
i <- grep('bio[0-9]$', names(envs))

# adds "bio0" to the names
editNames <- paste('bio', sapply(strsplit(names(envs)[i], 'bio'), function(x) x[2]), sep='0')

# adds these names to our bioclim raster
names(envs)[i] <- editNames

# subset by those variables selected (we could change which ones we want to look at here)
envs <- envs[[c('bio01', 'bio02', 'bio03', 'bio04', 'bio05', 'bio06', 'bio07', 'bio08', 'bio09', 'bio10', 'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18', 'bio19')]]

# extract environmental values at occ grid cells
locs.vals <- raster::extract(envs[[1]], occs[, c('longitude', 'latitude')])

# remove occs without environmental values
occs <- occs[!is.na(locs.vals), ]  

occs.xy <- occs[c('longitude', 'latitude')]
sp::coordinates(occs.xy) <- ~ longitude + latitude
bgExt <- mcp(occs.xy)

# Buffer size of the study extent polygon defined as 0.5 degrees
bgExt <- rgeos::gBuffer(bgExt, width = 0.5)

# crop the environmental rasters by the background extent shape
envsBgCrop <- raster::crop(envs, bgExt)

# mask the background extent shape from the cropped raster
envsBgMsk <- raster::mask(envsBgCrop, bgExt)

# sample random background points
bg.xy <- dismo::randomPoints(envsBgMsk, 10000)

# convert matrix output to data frame
bg.xy <- as.data.frame(bg.xy)  
colnames(bg.xy) <- c("longitude", "latitude")

# We also created a function for steps required to extract background points, so you can try multiple values at once
bg_points<- function(var){
        exbg.xy <- dismo::randomPoints(envsBgMsk, var) #create random points
        exbg.xy <- as.data.frame(exbg.xy) #convert to dataframe
        colnames(exbg.xy) <- c("longitude", "latitude")
        exbg_extract <- raster::extract(envsCrop, exbg.xy) # extract climate data
        exbg_extract <- as.data.frame(exbg_extract)
        exbg_SWD <- data.frame(cbind(Species = "background", exbg.xy, exbg_extract))
        #write csv
        write.csv(exbg_SWD, file = paste0("maxent_SWD/exbg_SWD", var, ".csv"), row.names = F)
        return(exbg_SWD)
}
vals <- c(750, 1000, 5000, 10000)
df_list <- lapply(vals, bg_points)

```

### Load environmental data

#### Using getData() function

```{r}
# no longer using code below - downloaded worldclim version 2 data from website: https://www.worldclim.org/data/worldclim21.html
#bioclim_data <- raster::getData("worldclim", var = "bio", res = 2.5) # downloading worldclim data

#read in worldclim version 2 data
rastlist <- list.files(path = here("data", "wc2.1_2.5m_bio"), pattern='.tif$', full.names=TRUE)
bioclim_data_v2 <- stack(rastlist)

#get spatial extent using Afrotropical ecoregion from WWF. Data downloaded from https://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world
afrotropical_region <- read_sf(dsn = here("data", "wwf_ecoregions", "official"), layer = "wwf_terr_ecos") %>% 
  filter(REALM == "AT")

#check crs
crs(afrotropical_region) # +proj=longlat +datum=WGS84 +no_defs 



# cropping to extent of Africa spatial polygon
bioclim_crop <- raster::crop(bioclim_data_v2, afrotropical_region)

# double checking to see they line up
plot(bioclim_crop[[1]])
plot(buffer_points, add = TRUE)

# extract environmental values at occ grid cells
locs.vals <- raster::extract(bioclim_crop[[1]], occs[, c('longitude', 'latitude')])
# remove occs without environmental values
occs_ <- occs[!is.na(locs.vals), ]
```

Find correlated bioclim variables
```{r}
bioclim_corr=layerStats(bioclim_crop, 'pearson', na.rm=T)
corr_matrix=bioclim_corr$'pearson correlation coefficient'
corr_matrix[!corr_matrix > 0.75, ]
```

### LAT/LONG AND RASTER METHOD

```{r}
# Save occurrence point lat/long
lion_cleaned <- #coord_incomplete(coord_imprecise(coord_impossible(coord_unlikely(myspecies_coords))))%>%
  myspecies_coords %>% 
  rename(lat = latitude) %>% # renaming lat/long so it works with projection later on?
  rename(long = longitude)

lat_long <- lion_cleaned %>%
  dplyr::select(lat, long)

# Save lat/long
write_csv(lat_long, "lion_coords.csv")

# Download full raster layer for each bioclim variable for Maxent (may use in conjunction with bias layer)
bio <- c(1:19) #vector for each bioclim variable
for(i in bio){
        # Maxent reads either csv files or a directory -> need to export each bioclim raster separately
        writeRaster(bioclim_crop[[i]], 
                    filename = paste0(here("data", "bioclim_v2", "bio"), i, ".asc"),
                    overwrite=TRUE)
}
```

### Land Cover Data

Downloaded from: http://due.esrin.esa.int/page_globcover.php

Reclassified following 

```{r}
#import downloaded ESA GlobCover 2009 file
land <- here("data", "Globcover2009_V2.3_Global_", "GLOBCOVER_L4_200901_200912_V2.3.tif")
land = raster(land)

#crop to Africa polygon
land_crop <- crop(land, africa_spatial)

# create land cover class values matrix for reclassification (see GlobCover2009_Legend.xls in downloaded cover data for id values and corresponding cover labels)
mtrx <- rbind(c(0, 21, 1), c(22, 91, 2), c(92, 101, 3), c(102, 121, 4), c(122, 161, 3),
              c(162, 171, 2), c(172, 181, 3), c(182, 260, 5)) #why don't values match spreadsheet?

#reclassify to broader categories: ag, grassland, forest, etc.
mtrx_test <- rbind(c(11, 14, 1), c(20, 30, 2), c(40, 50, 3), c(60, 70, 4), c(90, 100, 3),
              c(110, 120, 2), c(130, 140, 3), c(150, 160, 5)) #see if it works with matching values

land_rcl <- reclassify(land_crop, mtrx)

#crop to same extent as other layers for maxent
#land_rcl <- crop(land_rcl, bioclim_data, snap = "near") 
land_rcl <- crop(land_crop, bioclim_data, snap = "near") #changed land_rcl to land_crop for now since land_rcl isn't working
land_rcl <- raster::resample(land_rcl, bio_new[[1]], method = "ngb")

#write raster
writeRaster(land_rcl, "C:/Users/roshn/OneDrive/Desktop/emLab/hwc/maxent_SWD/bio_layers/bias_extent/esa_landcover.asc", format="ascii", overwrite = T)
```

### SPECIES WITH DATA METHOD

Extract environmental data for occurrence and background points for species with data format for use with Maxent Gui

```{r}
# need just lat/long values in two columns to used raster extract
occs.xy <- occs[c('longitude', 'latitude')]
bg.xy <- as.data.frame(bg_points) # using buffer points for now

# extracting values
occ_extract <- raster::extract(bioclim_crop, occs.xy)
bg_extract <- raster::extract(bioclim_crop, bg.xy)

# saving as data frames, adding in data
occ_extract_final <- as.data.frame(occ_extract) %>% 
  mutate(latitude = occs.xy$latitude) %>% # adding long/lat back in
  mutate(longitude = occs.xy$longitude) %>% 
  mutate(species = "Panthera leo") %>% # adding species column
  dplyr::select(species, longitude, latitude, 1:19) # using select() to re-order columns

bg_extract_final <- as.data.frame(bg_extract) %>% 
  mutate(x = bg.xy$x) %>% 
  mutate(y = bg.xy$y) %>% 
  mutate(species = "background") %>% 
  dplyr::select(species, x, y, 1:19) %>% 
  drop_na() # remove points with NA values

# save csvs
write_csv(occ_extract_final, "lion_occ_SWD.csv")
write_csv(bg_extract_final, "lion_bg_SWD.csv")
```


### Bias Layers

```{r}




```


### Future Climate Data

This is just the basic way to download it and extract values, we didn't do anything further than this.

#### Download predicted climate rasters

```{r}
future_data <- getData('CMIP5',
                       var = "bio",
                       res = 2.5, 
                       rcp = 85, 
                       model = 'AC', 
                       year = 70)

# need names to match up to our preset-time climate data
names(future_data) <- names(bioclim_data)

# cropping to Africa polygon (not sure if we really need to, but might make it go faster)
future_data_crop <- raster::crop(future_data, africa_spatial)
```

#### Extracting predicted climate variables

- Do we only need to do the occurrence points?
- does it need the long/lat again? Or does it want x/y?

```{r}
# extracting values
occ_future_extract <- raster::extract(future_data_crop, occs.xy)

# saving as data frame, add long/lat back in, re-order
occ_extract_future_final <- as.data.frame(occ_future_extract) #should this be occ_future_extract instead of occ_extract?
# %>% 
  # mutate(latitude = occs.xy$latitude) %>% 
  # mutate(longitude = occs.xy$longitude) %>% 
  # dplyr::select(longitude, latitude, 1:19)

# save csv
write_csv(occ_extract_future_final, "lion_future_SWD.csv")
```

At this point, the SWD data files can be used to make prediction in the Maxent GUI, or used for modeling in R.


### SDM modeling in R

You can also do SDM in R using the maxent() function from the dismo package instead of using Wallace or the Maxent GUI, but we didn't investigate this very much because we had trouble getting rJava to work.  If you decide to go this route and also have trouble with rJava, try downloading the current Java Developer Kit from oracle.com 

https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html


```{r}
 library(dismo)
#install.packages("rJava")
 library(rJava)
# 
#withhold 20% of the data for testing the model
 fold <- kfold(occ_extract_final, k = 5) # making 5 groups
 lion_test <- occ_extract_final[fold == 1, ] #20% data
 lion_train <- occ_extract_final[fold != 1, ] #80% data
# 
# # making x argument
 predictions <- occ_extract_final %>%
   dplyr::select(bio1:bio19)
# 
# making p argument (just lat/long, )
 occurrence <- occ_extract_final %>%
   dplyr::select(longitude, latitude)
# 
 occurrence <- as.vector(occurrence)
# 
# making a argument
# # background <-
# 
# #fit the maxent model
 model <- maxent()
```

